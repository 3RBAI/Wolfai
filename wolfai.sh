#!/bin/bash

# ======================================================
# ğŸº Wolf AI - Ø³ÙƒØ±Ø¨Øª ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±
# ======================================================
# Ø§Ù„Ù…Ø¤Ù„Ù: WOLF-AI
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 1.0.0
# ======================================================

# Ø£Ù„ÙˆØ§Ù† Ù„Ù„Ø¹Ø±Ø¶
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
GOLD='\033[0;33m'
NC='\033[0m' # No Color

# ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
WOLF_VERSION="1.0.0"
BASE_MODEL="meta-llama/Llama-3-70b"
WOLF_MODEL_NAME="wolf-ai-crown-${WOLF_VERSION}"
TRAINING_DIR="$HOME/wolf_ai_training"
DATASETS_DIR="$TRAINING_DIR/datasets"
MODELS_DIR="$TRAINING_DIR/models"
CHECKPOINTS_DIR="$TRAINING_DIR/checkpoints"
LOGS_DIR="$TRAINING_DIR/logs"
OUTPUT_DIR="$TRAINING_DIR/output"
MINDMAP_TEMPLATES="$TRAINING_DIR/mindmap_templates"
API_KEYS_DIR="$TRAINING_DIR/api_keys"
CONFIG_FILE="$TRAINING_DIR/wolf_config.json"
CUDA_VISIBLE_DEVICES="0,1,2,3" # ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø¹Ø¯Ø¯ Ø¨Ø·Ø§Ù‚Ø§Øª GPU Ø§Ù„Ù…ØªÙˆÙØ±Ø©

# Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
setup_directories() {
    echo -e "${BLUE}[*] Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨...${NC}"
    mkdir -p $DATASETS_DIR/{philosophy,poetry,programming,technical,frontend,mindmaps}
    mkdir -p $MODELS_DIR
    mkdir -p $CHECKPOINTS_DIR
    mkdir -p $LOGS_DIR
    mkdir -p $OUTPUT_DIR
    mkdir -p $MINDMAP_TEMPLATES
    mkdir -p $API_KEYS_DIR
    echo -e "${GREEN}[+] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
install_dependencies() {
    echo -e "${BLUE}[*] ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©...${NC}"
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø¸Ø§Ù…
    apt update && apt upgrade -y
    
    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    apt install -y build-essential python3-dev python3-pip git wget curl libopenmpi-dev
    apt install -y nvidia-cuda-toolkit nvidia-cuda-toolkit-gcc
    
    # ØªØ«Ø¨ÙŠØª Ø­Ø²Ù… Python
    pip3 install --upgrade pip
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    pip3 install transformers datasets accelerate peft bitsandbytes sentencepiece
    pip3 install deepspeed optimum flash-attn
    pip3 install wandb tensorboard matplotlib networkx pydot graphviz
    pip3 install huggingface_hub gradio flask fastapi uvicorn
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    pip3 install pandas numpy scipy scikit-learn nltk spacy
    python3 -m spacy download ar_core_news_lg
    python3 -m spacy download en_core_web_trf
    
    echo -e "${GREEN}[+] ØªÙ… ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©
download_datasets() {
    echo -e "${BLUE}[*] ØªÙ†Ø²ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ®ØµØµØ©...${NC}"
    
    # Ø§Ù„ÙÙ„Ø³ÙØ© ÙˆØ§Ù„Ø´Ø¹Ø±
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙÙ„Ø³ÙØ© ÙˆØ§Ù„Ø´Ø¹Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø¹Ø§Ù„Ù…ÙŠ...${NC}"
    git clone https://github.com/OpenArabic/1000AH.git $DATASETS_DIR/philosophy/arabic_philosophy
    git clone https://github.com/OpenITI/OpenITI.git $DATASETS_DIR/philosophy/openiti
    wget -P $DATASETS_DIR/poetry https://archive.org/download/PoetryFoundationData/PoetryFoundationData.zip
    unzip $DATASETS_DIR/poetry/PoetryFoundationData.zip -d $DATASETS_DIR/poetry/
    
    # Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ©
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ©...${NC}"
    git clone https://github.com/facebookresearch/CodeLlama.git $DATASETS_DIR/programming/code_llama
    git clone https://github.com/github/CodeSearchNet.git $DATASETS_DIR/programming/code_search_net
    git clone https://github.com/TheAlgorithms/Python.git $DATASETS_DIR/programming/algorithms_python
    git clone https://github.com/TheAlgorithms/JavaScript.git $DATASETS_DIR/programming/algorithms_js
    git clone https://github.com/kamranahmedse/developer-roadmap.git $DATASETS_DIR/programming/developer_roadmap
    
    # ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„ØªØµÙ…ÙŠÙ…...${NC}"
    git clone https://github.com/vercel/next.js.git $DATASETS_DIR/frontend/nextjs
    git clone https://github.com/facebook/react.git $DATASETS_DIR/frontend/react
    git clone https://github.com/tailwindlabs/tailwindcss.git $DATASETS_DIR/frontend/tailwindcss
    git clone https://github.com/shadcn-ui/ui.git $DATASETS_DIR/frontend/shadcn
    
    # Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...${NC}"
    git clone https://github.com/markmap/markmap.git $DATASETS_DIR/mindmaps/markmap
    
    # ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Hugging Face
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Hugging Face...${NC}"
    python3 -c "
from datasets import load_dataset
from huggingface_hub import login

# ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù‡Ù†Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§ØµØ©
# login('your_huggingface_token')

# ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©
datasets = [
    ('allenai/c4', 'ar'),
    ('allenai/c4', 'en'),
    ('Abirate/arabic_poetry', 'default'),
    ('wikimedia/wikipedia', 'arwiki'),
    ('togethercomputer/RedPajama-Data-1T', 'default'),
    ('bigcode/the-stack', 'data'),
    ('HuggingFaceH4/stack-exchange-preferences', 'default')
]

for dataset_name, subset in datasets:
    try:
        print(f'ØªÙ†Ø²ÙŠÙ„ {dataset_name} ({subset})...')
        if subset == 'default':
            dataset = load_dataset(dataset_name, split='train')
        else:
            dataset = load_dataset(dataset_name, subset, split='train')
        print(f'ØªÙ… ØªÙ†Ø²ÙŠÙ„ {len(dataset)} Ø¹Ù†ØµØ± Ù…Ù† {dataset_name}')
        # Ø­ÙØ¸ Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„Ù„ØªØ­Ù‚Ù‚
        sample = dataset.shuffle().select(range(min(100, len(dataset))))
        sample.save_to_disk(f'$DATASETS_DIR/{dataset_name.split('/')[-1]}_{subset}_sample')
    except Exception as e:
        print(f'Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø²ÙŠÙ„ {dataset_name}: {e}')
"
    
    echo -e "${GREEN}[+] ØªÙ… ØªÙ†Ø²ÙŠÙ„ ÙˆØ¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
prepare_data() {
    echo -e "${BLUE}[*] Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    cat > $TRAINING_DIR/prepare_data.py << 'EOL'
import os
import json
import random
import re
import pandas as pd
import numpy as np
from datasets import load_dataset, Dataset, DatasetDict
from transformers import AutoTokenizer

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-70b")

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
base_dir = os.environ.get("TRAINING_DIR")
datasets_dir = os.path.join(base_dir, "datasets")
output_dir = os.path.join(base_dir, "output")

# Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ù„Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª
instruction_templates = {
    "philosophy": [
        "ÙÙƒØ± ÙƒÙÙŠÙ„Ø³ÙˆÙ ÙˆØ£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ: {question}",
        "Ù†Ø§Ù‚Ø´ Ø§Ù„Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ÙÙ„Ø³ÙÙŠ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† ÙˆØ¬Ù‡Ø© Ù†Ø¸Ø± {philosopher}: {concept}",
        "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† ÙˆØ¬Ù‡Ø§Øª Ø§Ù„Ù†Ø¸Ø± Ø§Ù„ÙÙ„Ø³ÙÙŠØ© Ù„Ù€ {philosopher1} Ùˆ {philosopher2} Ø­ÙˆÙ„ {topic}",
    ],
    "poetry": [
        "Ø§ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© Ø¹Ù† {topic} Ø¨Ø£Ø³Ù„ÙˆØ¨ {style}",
        "Ø£ÙƒÙ…Ù„ Ø§Ù„Ù‚ØµÙŠØ¯Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ù†ÙØ³ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨: {poem_start}",
        "Ø§ÙƒØªØ¨ Ù‚ØµÙŠØ¯Ø© ØªØ¹Ø¨Ø± Ø¹Ù† {emotion} Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³ØªØ¹Ø§Ø±Ø§Øª Ù…Ù† {domain}",
    ],
    "programming": [
        "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ {language} Ù„Ù€ {task}",
        "Ø§Ø´Ø±Ø­ ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: {algorithm}",
        "Ø­Ø³Ù‘Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ù† Ø­ÙŠØ« {aspect}: {code}",
        "Ù‚Ù… Ø¨ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ§Ù„ÙŠ: {code}",
    ],
    "technical": [
        "Ø§Ø´Ø±Ø­ Ø¨Ø§Ù„ØªÙØµÙŠÙ„ ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ {technology}",
        "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† {tech1} Ùˆ {tech2} Ù…Ù† Ø­ÙŠØ« {aspects}",
        "Ø§ÙƒØªØ¨ ØªÙˆØ«ÙŠÙ‚Ù‹Ø§ ØªÙ‚Ù†ÙŠÙ‹Ø§ Ù„Ù€ {api_name} ÙŠØ´Ø±Ø­ {functionality}",
    ],
    "frontend": [
        "ØµÙ…Ù… ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù€ {website_type} Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… {framework}",
        "Ø§ÙƒØªØ¨ ÙƒÙˆØ¯ HTML Ùˆ CSS Ùˆ JavaScript Ù„Ø¥Ù†Ø´Ø§Ø¡ {component}",
        "Ø§Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ {framework} Ù…Ù† Ø®Ù„Ø§Ù„ {technique}",
    ],
    "mindmap": [
        "Ø£Ù†Ø´Ø¦ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© ØªÙˆØ¶Ø­ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† {concepts}",
        "Ù‚Ù… Ø¨ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ©: {information}",
        "ØµÙ…Ù… Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© Ù„Ø´Ø±Ø­ {topic} Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©",
    ]
}

# Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
def create_training_examples(category, num_examples=1000):
    templates = instruction_templates[category]
    examples = []
    
    # Ù‚ÙˆØ§Ù…ÙŠØ³ Ù„Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
    replacements = {
        "philosophy": {
            "question": ["Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©ØŸ", "Ù…Ø§ Ù…Ø¹Ù†Ù‰ Ø§Ù„ÙˆØ¬ÙˆØ¯ØŸ", "ÙƒÙŠÙ Ù†Ø¹Ø±Ù Ù…Ø§ Ù†Ø¹Ø±ÙÙ‡ØŸ", "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©ØŸ"],
            "philosopher": ["Ø£Ø±Ø³Ø·Ùˆ", "ÙƒØ§Ù†Ø·", "Ù†ÙŠØªØ´Ù‡", "Ø§Ø¨Ù† Ø±Ø´Ø¯", "Ø§Ù„ÙØ§Ø±Ø§Ø¨ÙŠ", "Ø§Ø¨Ù† Ø³ÙŠÙ†Ø§", "Ø¯ÙŠÙƒØ§Ø±Øª"],
            "concept": ["Ø§Ù„ÙˆØ¬ÙˆØ¯ÙŠØ©", "Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©", "Ø§Ù„Ù…Ø§Ø¯ÙŠØ©", "Ø§Ù„Ø­ØªÙ…ÙŠØ©", "Ø§Ù„Ø­Ø±ÙŠØ©", "Ø§Ù„Ø£Ø®Ù„Ø§Ù‚"],
            "topic": ["Ø§Ù„Ø£Ø®Ù„Ø§Ù‚", "Ø§Ù„Ù…Ø¹Ø±ÙØ©", "Ø§Ù„ÙˆØ¬ÙˆØ¯", "Ø§Ù„Ø¬Ù…Ø§Ù„", "Ø§Ù„Ø¹Ø¯Ø§Ù„Ø©", "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©"]
        },
        "poetry": {
            "topic": ["Ø§Ù„Ø­Ø¨", "Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©", "Ø§Ù„Ø­ÙŠØ§Ø©", "Ø§Ù„Ù…ÙˆØª", "Ø§Ù„Ø­Ø±ÙŠØ©", "Ø§Ù„ÙˆØ·Ù†", "Ø§Ù„ØºØ±Ø¨Ø©"],
            "style": ["ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ", "Ø­Ø±", "Ø­Ø¯ÙŠØ«", "Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ", "ØµÙˆÙÙŠ", "Ù…Ù„Ø­Ù…ÙŠ"],
            "poem_start": ["Ø¹Ù„Ù‰ Ù‚Ù…Ù… Ø§Ù„Ø¬Ø¨Ø§Ù„ ØªØ³ÙƒÙ† Ø§Ù„Ø£Ø­Ù„Ø§Ù…", "ÙÙŠ Ù„ÙŠÙ„Ù Ø·ÙˆÙŠÙ„ ÙˆØ§Ù„Ù†Ø¬ÙˆÙ… ØªÙ„Ù…Ø¹", "Ø³Ø£Ù„ØªÙ Ø§Ù„Ø¨Ø­Ø± Ø¹Ù† Ø³Ø± Ø§Ù„Ø­ÙŠØ§Ø©"],
            "emotion": ["Ø§Ù„ÙØ±Ø­", "Ø§Ù„Ø­Ø²Ù†", "Ø§Ù„Ø´ÙˆÙ‚", "Ø§Ù„ØºØ¶Ø¨", "Ø§Ù„Ø£Ù…Ù„", "Ø§Ù„ÙŠØ£Ø³"],
            "domain": ["Ø§Ù„Ø·Ø¨ÙŠØ¹Ø©", "Ø§Ù„ÙØ¶Ø§Ø¡", "Ø§Ù„Ø¨Ø­Ø±", "Ø§Ù„ØµØ­Ø±Ø§Ø¡", "Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", "Ø§Ù„ØªØ§Ø±ÙŠØ®"]
        },
        "programming": {
            "language": ["Python", "JavaScript", "TypeScript", "Rust", "Go", "C++", "Java"],
            "task": ["ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ", "ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±", "Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù…", "Ø¨Ù†Ø§Ø¡ API"],
            "algorithm": ["Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ", "Ø§Ù„ÙØ±Ø² Ø§Ù„Ø³Ø±ÙŠØ¹", "Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¯ÙŠÙƒØ³ØªØ±Ø§", "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©", "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚"],
            "aspect": ["Ø§Ù„Ø£Ø¯Ø§Ø¡", "Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©", "Ø§Ù„Ø£Ù…Ø§Ù†", "Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹"],
            "code": ["def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"]
        },
        "technical": {
            "technology": ["Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©", "blockchain", "Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©", "Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"],
            "tech1": ["React", "Angular", "TensorFlow", "PyTorch", "Docker", "Kubernetes"],
            "tech2": ["Vue", "Svelte", "Keras", "JAX", "Podman", "Nomad"],
            "aspects": ["Ø§Ù„Ø£Ø¯Ø§Ø¡", "Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…", "Ø§Ù„Ù…Ø¬ØªÙ…Ø¹", "Ø§Ù„ØªÙˆØ«ÙŠÙ‚", "Ø§Ù„Ø£Ù…Ø§Ù†"],
            "api_name": ["REST API", "GraphQL", "gRPC", "WebSocket", "WebRTC"],
            "functionality": ["Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ø§Ù„ØªØ®Ø²ÙŠÙ†", "Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª", "Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"]
        },
        "frontend": {
            "website_type": ["Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ", "Ù…Ù†ØµØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ©", "Ø´Ø¨ÙƒØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©", "Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ…", "Ù…ÙˆÙ‚Ø¹ Ø£Ø®Ø¨Ø§Ø±"],
            "framework": ["React", "Vue", "Angular", "Svelte", "Next.js", "Nuxt.js"],
            "component": ["Ù‚Ø§Ø¦Ù…Ø© Ù…Ù†Ø³Ø¯Ù„Ø©", "Ù†Ù…ÙˆØ°Ø¬ ØªØ³Ø¬ÙŠÙ„", "Ø´Ø±ÙŠØ· ØªÙ†Ù‚Ù„", "Ø¨Ø·Ø§Ù‚Ø© Ù…Ù†ØªØ¬", "Ù…Ø®Ø·Ø· Ø¨ÙŠØ§Ù†ÙŠ"],
            "technique": ["ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ÙƒÙˆØ¯", "Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª", "Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ£Ø®Ø±", "SSR", "ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±"]
        },
        "mindmap": {
            "concepts": ["Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆÙŠØ¨", "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ", "Ø§Ù„ÙÙ„Ø³ÙØ©"],
            "information": ["Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©", "ØªØ§Ø±ÙŠØ® Ø§Ù„ÙÙ„Ø³ÙØ©", "ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"],
            "topic": ["Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©", "ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆÙŠØ¨", "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª", "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ"]
        }
    }
    
    for _ in range(num_examples):
        template = random.choice(templates)
        filled_template = template
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø§Ù„Ø¨
        for key, values in replacements[category].items():
            pattern = "{" + key + "}"
            if pattern in filled_template:
                replacement = random.choice(values)
                filled_template = filled_template.replace(pattern, replacement)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙˆÙ‡Ù…ÙŠØ© (Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ø§Ø­Ù‚Ù‹Ø§)
        response = f"Ù‡Ø°Ù‡ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬ÙŠØ© Ù„Ù€: {filled_template}"
        
        examples.append({
            "instruction": filled_template,
            "response": response,
            "category": category
        })
    
    return examples

# Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ©
def merge_datasets():
    all_examples = []
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Ù„ÙƒÙ„ ÙØ¦Ø©
    for category in instruction_templates.keys():
        examples = create_training_examples(category, num_examples=5000)
        all_examples.extend(examples)
    
    # Ø®Ù„Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    random.shuffle(all_examples)
    
    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªØ¯Ø±ÙŠØ¨ ÙˆØªØ­Ù‚Ù‚ ÙˆØ§Ø®ØªØ¨Ø§Ø±
    train_size = int(0.8 * len(all_examples))
    val_size = int(0.1 * len(all_examples))
    
    train_examples = all_examples[:train_size]
    val_examples = all_examples[train_size:train_size+val_size]
    test_examples = all_examples[train_size+val_size:]
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    train_dataset = Dataset.from_pandas(pd.DataFrame(train_examples))
    val_dataset = Dataset.from_pandas(pd.DataFrame(val_examples))
    test_dataset = Dataset.from_pandas(pd.DataFrame(test_examples))
    
    # Ø¥Ù†Ø´Ø§Ø¡ DatasetDict
    dataset_dict = DatasetDict({
        'train': train_dataset,
        'validation': val_dataset,
        'test': test_dataset
    })
    
    # Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    dataset_dict.save_to_disk(os.path.join(output_dir, "wolf_ai_dataset"))
    
    print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù€ {len(train_examples)} Ù…Ø«Ø§Ù„ ØªØ¯Ø±ÙŠØ¨ØŒ {len(val_examples)} Ù…Ø«Ø§Ù„ ØªØ­Ù‚Ù‚ØŒ Ùˆ {len(test_examples)} Ù…Ø«Ø§Ù„ Ø§Ø®ØªØ¨Ø§Ø±")
    
    return dataset_dict

# ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
if __name__ == "__main__":
    print("Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    dataset = merge_datasets()
    print("Ø§ÙƒØªÙ…Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    echo -e "${YELLOW}[*] ØªÙ†ÙÙŠØ° Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...${NC}"
    export TRAINING_DIR=$TRAINING_DIR
    python3 $TRAINING_DIR/prepare_data.py
    
    echo -e "${GREEN}[+] ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
create_mindmap_templates() {
    echo -e "${BLUE}[*] Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ù„ØªÙˆÙ„ÙŠØ¯ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    cat > $TRAINING_DIR/create_mindmap_templates.py << 'EOL'
import os
import json
import random
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap
import numpy as np

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
base_dir = os.environ.get("TRAINING_DIR")
mindmap_dir = os.path.join(base_dir, "mindmap_templates")

# Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù„ÙˆØ§Ù† Ù…ØªØ¯Ø±Ø¬Ø© Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
colors = ["#D4AF37", "#8A2BE2", "#2D8CFF", "#FF4500", "#32CD32"]
cmap = LinearSegmentedColormap.from_list("wolf_colors", colors, N=100)

# Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
def create_mindmap_template(template_name, num_nodes=20, max_depth=3):
    # Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù…ÙˆØ¬Ù‡
    G = nx.DiGraph()
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø¬Ø°Ø±
    G.add_node(0, label=template_name, depth=0)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§Ù„Ø­ÙˆØ§Ù
    current_node_id = 1
    for depth in range(1, max_depth + 1):
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚Ø¯ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆÙ‰
        nodes_at_this_level = random.randint(2, 5) if depth == 1 else random.randint(1, 3)
        
        # Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ø£Ù… Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© (Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³Ø§Ø¨Ù‚)
        potential_parents = [n for n, d in G.nodes(data=True) if d['depth'] == depth - 1]
        
        for _ in range(nodes_at_this_level * len(potential_parents)):
            parent = random.choice(potential_parents)
            G.add_node(current_node_id, label=f"Node {current_node_id}", depth=depth)
            G.add_edge(parent, current_node_id)
            current_node_id += 1
    
    # Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ÙƒÙ…Ù„Ù JSON
    graph_data = {
        "nodes": [{"id": n, "label": G.nodes[n]["label"], "depth": G.nodes[n]["depth"]} for n in G.nodes()],
        "edges": [{"source": u, "target": v} for u, v in G.edges()]
    }
    
    with open(os.path.join(mindmap_dir, f"{template_name}.json"), "w", encoding="utf-8") as f:
        json.dump(graph_data, f, ensure_ascii=False, indent=2)
    
    # Ø±Ø³Ù… ÙˆØ­ÙØ¸ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ© ÙƒØµÙˆØ±Ø©
    plt.figure(figsize=(12, 8))
    pos = nx.nx_agraph.graphviz_layout(G, prog="twopi", root=0)
    
    # Ø±Ø³Ù… Ø§Ù„Ø¹Ù‚Ø¯ Ø¨Ø£Ù„ÙˆØ§Ù† Ù…Ø®ØªÙ„ÙØ© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù‚
    max_depth_val = max(nx.get_node_attributes(G, 'depth').values())
    node_colors = [cmap(G.nodes[n]['depth'] / max_depth_val) for n in G.nodes()]
    
    nx.draw(G, pos, with_labels=True, labels=nx.get_node_attributes(G, 'label'),
            node_color=node_colors, node_size=1500, font_size=10, font_weight='bold',
            edge_color='gray', width=1.5, alpha=0.8, arrows=True)
    
    plt.title(f"Wolf AI Mindmap Template: {template_name}", fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(mindmap_dir, f"{template_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()
    
    return graph_data

# Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ù…ØªÙ†ÙˆØ¹Ø©
template_topics = [
    "Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„ÙÙ„Ø³ÙÙŠ",
    "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¹Ø±",
    "Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©",
    "ØªØµÙ…ÙŠÙ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª",
    "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
    "Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©",
    "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©",
    "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©",
    "Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©",
    "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²"
]

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨
for topic in template_topics:
    print(f"Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù„Ø¨ Ù„Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ©: {topic}")
    create_mindmap_template(topic, num_nodes=random.randint(15, 30), max_depth=random.randint(3, 5))

print(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(template_topics)} Ù‚Ø§Ù„Ø¨ Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    echo -e "${YELLOW}[*] Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...${NC}"
    export TRAINING_DIR=$TRAINING_DIR
    python3 $TRAINING_DIR/create_mindmap_templates.py
    
    echo -e "${GREEN}[+] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
download_base_model() {
    echo -e "${BLUE}[*] ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    cat > $TRAINING_DIR/download_model.py << 'EOL'
from huggingface_hub import snapshot_download
import os

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
base_dir = os.environ.get("TRAINING_DIR")
models_dir = os.path.join(base_dir, "models")
model_name = os.environ.get("BASE_MODEL")

# ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print(f"ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {model_name}")
model_path = snapshot_download(
    repo_id=model_name,
    local_dir=os.path.join(models_dir, model_name.split("/")[-1]),
    ignore_patterns=["*.safetensors", "*.bin"] if os.environ.get("QUANTIZE") == "true" else None
)

print(f"ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰: {model_path}")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    echo -e "${YELLOW}[*] ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ${BASE_MODEL}...${NC}"
    export TRAINING_DIR=$TRAINING_DIR
    export BASE_MODEL=$BASE_MODEL
    export QUANTIZE="true"  # ØªØ¹ÙŠÙŠÙ† Ù„Ù€ true Ù„Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹ (Ø¨Ø¯ÙˆÙ† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©)
    python3 $TRAINING_DIR/download_model.py
    
    echo -e "${GREEN}[+] ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… QLoRA
finetune_model() {
    echo -e "${BLUE}[*] Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… QLoRA...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù„Ù„ØªØ¯Ø±ÙŠØ¨
    cat > $CONFIG_FILE << EOL
{
    "base_model": "${BASE_MODEL}",
    "model_name": "${WOLF_MODEL_NAME}",
    "dataset_path": "${OUTPUT_DIR}/wolf_ai_dataset",
    "output_dir": "${MODELS_DIR}/${WOLF_MODEL_NAME}",
    "checkpoints_dir": "${CHECKPOINTS_DIR}",
    "training_params": {
        "num_train_epochs": 3,
        "per_device_train_batch_size": 4,
        "per_device_eval_batch_size": 4,
        "gradient_accumulation_steps": 8,
        "learning_rate": 2e-5,
        "weight_decay": 0.01,
        "warmup_ratio": 0.03,
        "max_grad_norm": 0.3,
        "lora_r": 64,
        "lora_alpha": 16,
        "lora_dropout": 0.1,
        "max_seq_length": 2048,
        "fp16": true
    }
}
EOL
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    cat > $TRAINING_DIR/finetune_model.py << 'EOL'
import os
import json
import torch
from datasets import load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
    TaskType
)
import wandb

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
config_file = os.environ.get("CONFIG_FILE")
with open(config_file, "r") as f:
    config = json.load(f)

# ØªÙ‡ÙŠØ¦Ø© wandb Ù„Ù„ØªØªØ¨Ø¹
wandb.init(project="wolf-ai-training", name=config["model_name"])

# ØªÙƒÙˆÙŠÙ† BitsAndBytes Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø¯Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
print(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {config['base_model']}")
model = AutoModelForCausalLM.from_pretrained(
    config["base_model"],
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(config["base_model"], trust_remote_code=True)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
model = prepare_model_for_kbit_training(model)

# ØªÙƒÙˆÙŠÙ† LoRA
lora_config = LoraConfig(
    r=config["training_params"]["lora_r"],
    lora_alpha=config["training_params"]["lora_alpha"],
    lora_dropout=config["training_params"]["lora_dropout"],
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# ØªØ·Ø¨ÙŠÙ‚ LoRA Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = get_peft_model(model, lora_config)

# ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print(f"ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: {config['dataset_path']}")
dataset = load_from_disk(config["dataset_path"])

# ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
def preprocess_function(examples):
    # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
    prompts = []
    for instruction, response in zip(examples["instruction"], examples["response"]):
        prompt = f"### ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instruction}\n\n### Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n{response}"
        prompts.append(prompt)
    
    # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ
    tokenized = tokenizer(
        prompts,
        truncation=True,
        max_length=config["training_params"]["max_seq_length"],
        padding="max_length"
    )
    
    return tokenized

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
tokenized_datasets = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["instruction", "response", "category"]
)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…ÙÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_args = TrainingArguments(
    output_dir=config["output_dir"],
    num_train_epochs=config["training_params"]["num_train_epochs"],
    per_device_train_batch_size=config["training_params"]["per_device_train_batch_size"],
    per_device_eval_batch_size=config["training_params"]["per_device_eval_batch_size"],
    gradient_accumulation_steps=config["training_params"]["gradient_accumulation_steps"],
    learning_rate=config["training_params"]["learning_rate"],
    weight_decay=config["training_params"]["weight_decay"],
    warmup_ratio=config["training_params"]["warmup_ratio"],
    max_grad_norm=config["training_params"]["max_grad_norm"],
    fp16=config["training_params"]["fp16"],
    logging_steps=10,
    save_steps=100,
    eval_steps=100,
    save_total_limit=3,
    evaluation_strategy="steps",
    load_best_model_at_end=True,
    report_to="wandb",
    save_safetensors=True,
    lr_scheduler_type="cosine",
    seed=42
)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
)

# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print("Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
trainer.train()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
print(f"Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙÙŠ: {config['output_dir']}")
trainer.save_model(config["output_dir"])
tokenizer.save_pretrained(config["output_dir"])

# Ø­ÙØ¸ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
with open(os.path.join(config["output_dir"], "wolf_config.json"), "w") as f:
    json.dump(config, f, indent=2)

print("Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    echo -e "${YELLOW}[*] Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...${NC}"
    export CONFIG_FILE=$CONFIG_FILE
    export CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES
    python3 $TRAINING_DIR/finetune_model.py
    
    echo -e "${GREEN}[+] ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
enhance_mindmap_capabilities() {
    echo -e "${BLUE}[*] ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    cat > $TRAINING_DIR/enhance_mindmap.py << 'EOL'
import os
import json
import torch
import random
from datasets import Dataset, load_from_disk
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import PeftModel, PeftConfig

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
config_file = os.environ.get("CONFIG_FILE")
with open(config_file, "r") as f:
    config = json.load(f)

# ØªØ­Ù…ÙŠÙ„ Ù‚ÙˆØ§Ù„Ø¨ Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
mindmap_dir = os.path.join(os.environ.get("TRAINING_DIR"), "mindmap_templates")
mindmap_templates = []
for filename in os.listdir(mindmap_dir):
    if filename.endswith(".json"):
        with open(os.path.join(mindmap_dir, filename), "r") as f:
            template = json.load(f)
            template["name"] = filename.replace(".json", "")
            mindmap_templates.append(template)

# Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© ØªØ¯Ø±ÙŠØ¨ Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
def create_mindmap_examples(num_examples=500):
    examples = []
    
    topics = [
        "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©", "Ø§Ù„ÙÙ„Ø³ÙØ©", "Ø§Ù„Ø´Ø¹Ø±", "ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆÙŠØ¨",
        "Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©", "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚", "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©",
        "Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©", "Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²", "Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ",
        "Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª", "Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª"
    ]
    
    for _ in range(num_examples):
        topic = random.choice(topics)
        template = random.choice(mindmap_templates)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†ØµÙŠ Ù„Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
        mindmap_text = f"# Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ©: {topic}\n\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù‚Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ù‚
        nodes_by_depth = {}
        for node in template["nodes"]:
            depth = node["depth"]
            if depth not in nodes_by_depth:
                nodes_by_depth[depth] = []
            nodes_by_depth[depth].append(node)
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø§Ù„Ù†ØµÙŠØ©
        for depth in sorted(nodes_by_depth.keys()):
            for node in nodes_by_depth[depth]:
                indent = "  " * depth
                node_label = f"{topic} - {node['label']}" if depth == 0 else f"ÙØ±Ø¹ {node['id']}: {node['label']}"
                mindmap_text += f"{indent}- {node_label}\n"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        instruction = f"Ø£Ù†Ø´Ø¦ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© Ù„Ø´Ø±Ø­ {topic} Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ù†Ø¸Ù…Ø© ÙˆÙ…ØªØ±Ø§Ø¨Ø·Ø©."
        response = f"ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© Ù„Ø´Ø±Ø­ {topic}:\n\n```mindmap\n{mindmap_text}\n```"
        
        examples.append({
            "instruction": instruction,
            "response": response,
            "category": "mindmap"
        })
    
    return examples

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
mindmap_examples = create_mindmap_examples(num_examples=1000)
mindmap_dataset = Dataset.from_pandas(pd.DataFrame(mindmap_examples))

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
model_path = config["output_dir"]
print(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ù†: {model_path}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
peft_config = PeftConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(
    peft_config.base_model_name_or_path,
    device_map="auto",
    torch_dtype=torch.float16
)
model = PeftModel.from_pretrained(model, model_path)

# ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
def preprocess_function(examples):
    prompts = []
    for instruction, response in zip(examples["instruction"], examples["response"]):
        prompt = f"### ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instruction}\n\n### Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n{response}"
        prompts.append(prompt)
    
    tokenized = tokenizer(
        prompts,
        truncation=True,
        max_length=config["training_params"]["max_seq_length"],
        padding="max_length"
    )
    
    return tokenized

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
tokenized_dataset = mindmap_dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=["instruction", "response", "category"]
)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…ÙÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_args = TrainingArguments(
    output_dir=os.path.join(config["output_dir"], "mindmap_enhanced"),
    num_train_epochs=2,
    per_device_train_batch_size=config["training_params"]["per_device_train_batch_size"],
    learning_rate=1e-5,
    weight_decay=0.01,
    warmup_ratio=0.03,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    report_to="none",
    save_safetensors=True
)

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print("Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...")
trainer.train()

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†
print(f"Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù† ÙÙŠ: {os.path.join(config['output_dir'], 'mindmap_enhanced')}")
trainer.save_model(os.path.join(config["output_dir"], "mindmap_enhanced"))

print("Ø§ÙƒØªÙ…Ù„ ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­!")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    echo -e "${YELLOW}[*] ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ©...${NC}"
    export CONFIG_FILE=$CONFIG_FILE
    export TRAINING_DIR=$TRAINING_DIR
    python3 $TRAINING_DIR/enhance_mindmap.py
    
    echo -e "${GREEN}[+] ØªÙ… ØªØ­Ø³ÙŠÙ† Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø°Ù‡Ù†ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
evaluate_model() {
    echo -e "${BLUE}[*] ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    cat > $TRAINING_DIR/evaluate_model.py << 'EOL'
import os
import json
import torch
import pandas as pd
import numpy as np
from datasets import load_from_disk
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig
from tqdm import tqdm

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
config_file = os.environ.get("CONFIG_FILE")
with open(config_file, "r") as f:
    config = json.load(f)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
model_path = config["output_dir"]
print(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ù†: {model_path}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
peft_config = PeftConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(
    peft_config.base_model_name_or_path,
    device_map="auto",
    torch_dtype=torch.float16
)
model = PeftModel.from_pretrained(model, model_path)

# ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
dataset = load_from_disk(config["dataset_path"])
test_dataset = dataset["test"]

# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
results = []

for i, example in enumerate(tqdm(test_dataset, desc="ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")):
    if i >= 100:  # ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ 100 Ù…Ø«Ø§Ù„ ÙÙ‚Ø· Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
        break
    
    instruction = example["instruction"]
    expected_response = example["response"]
    category = example["category"]
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
    prompt = f"### ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instruction}\n\n### Ø§Ø³ØªØ¬Ø§Ø¨Ø©:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
    
    # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = generated_text.split("### Ø§Ø³ØªØ¬Ø§Ø¨Ø©:")[1].strip()
    
    # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
    # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§ Ù…Ø«Ù„ BLEU Ø£Ùˆ ROUGE
    length_ratio = len(response) / max(1, len(expected_response))
    
    results.append({
        "category": category,
        "instruction": instruction,
        "expected_response": expected_response,
        "generated_response": response,
        "length_ratio": length_ratio
    })

# ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
df = pd.DataFrame(results)
avg_length_ratio = df["length_ratio"].mean()
category_stats = df.groupby("category")["length_ratio"].mean()

# Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
output_file = os.path.join(config["output_dir"], "evaluation_results.json")
with open(output_file, "w") as f:
    json.dump({
        "avg_length_ratio": avg_length_ratio,
        "category_stats": category_stats.to_dict(),
        "examples": results[:10]  # Ø­ÙØ¸ 10 Ø£Ù…Ø«Ù„Ø© ÙÙ‚Ø· Ù„Ù„ØªÙˆØ¶ÙŠØ­
    }, f, indent=2, ensure_ascii=False)

print(f"ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙÙŠ: {output_file}")
print(f"Ù…ØªÙˆØ³Ø· Ù†Ø³Ø¨Ø© Ø§Ù„Ø·ÙˆÙ„: {avg_length_ratio}")
print("Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©:")
print(category_stats)

print("Ø§ÙƒØªÙ…Ù„ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    echo -e "${YELLOW}[*] ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...${NC}"
    export CONFIG_FILE=$CONFIG_FILE
    python3 $TRAINING_DIR/evaluate_model.py
    
    echo -e "${GREEN}[+] ØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
merge_and_optimize() {
    echo -e "${BLUE}[*] Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†
    cat > $TRAINING_DIR/merge_and_optimize.py << 'EOL'
import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
config_file = os.environ.get("CONFIG_FILE")
with open(config_file, "r") as f:
    config = json.load(f)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
model_path = config["output_dir"]
print(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ù†: {model_path}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
peft_config = PeftConfig.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ...")
base_model = AutoModelForCausalLM.from_pretrained(
    peft_config.base_model_name_or_path,
    device_map="auto",
    torch_dtype=torch.float16
)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
print("ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨...")
model = PeftModel.from_pretrained(base_model, model_path)

# Ø¯Ù…Ø¬ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
print("Ø¯Ù…Ø¬ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
merged_model = model.merge_and_unload()

# ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
merged_model.config.pretraining_tp = 1

# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬
final_model_path = os.path.join(config["output_dir"], "merged")
print(f"Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ: {final_model_path}")
merged_model.save_pretrained(final_model_path, safe_serialization=True)
tokenizer.save_pretrained(final_model_path)

# Ø­ÙØ¸ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
final_config = config.copy()
final_config["model_type"] = "merged"
final_config["merge_date"] = "2025-06-10"  # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ§Ø±ÙŠØ® Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©

with open(os.path.join(final_model_path, "wolf_config.json"), "w") as f:
    json.dump(final_config, f, indent=2, ensure_ascii=False)

print("Ø§ÙƒØªÙ…Ù„ Ø¯Ù…Ø¬ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
EOL
    
    # ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†
    echo -e "${YELLOW}[*] Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...${NC}"
    export CONFIG_FILE=$CONFIG_FILE
    python3 $TRAINING_DIR/merge_and_optimize.py
    
    echo -e "${GREEN}[+] ØªÙ… Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ù†Ø¬Ø§Ø­${NC}"
}

# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© API Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
create_api() {
    echo -e "${BLUE}[*] Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© API Ù„Ù„Ù†Ù…ÙˆØ°Ø¬...${NC}"
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª ÙˆØ§Ø¬Ù‡Ø© API
    cat > $TRAINING_DIR/wolf_api.py << 'EOL'
import os
import json
import torch
import uuid
import time
import base64
import hashlib
import hmac
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException, Depends, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
config_file = os.environ.get("CONFIG_FILE")
with open(config_file, "r") as f:
    config = json.load(f)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬
model_path = os.path.join(config["output_dir"], "merged")
print(f"ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ù…Ø¬ Ù…Ù†: {model_path}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto",
    torch_dtype=torch.float16
)

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ FastAPI
app = FastAPI(
    title="Wolf AI API",
    description="ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Wolf AI",
    version="1.0.0"
)

# Ø¥Ø¶Ø§ÙØ© CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø·Ù„Ø¨
class CompletionRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    stream: bool = False
    include_mindmap: bool = False

# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
class CompletionResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Dict[str, int]
    mindmap: Optional[Dict[str, Any]] = None

# Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…ÙØ§ØªÙŠØ­
API_KEYS = {}

# Ø¯Ø§Ù„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ API
def generate_api_key(user_id: str, expiry_days: int = 365) -> str:
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    random_key = uuid.uuid4().hex
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆÙ‚ÙŠØ¹ HMAC
    secret = os.environ.get("WOLF_API_SECRET", "wolf_ai_secret_key")
    signature = hmac.new(
        secret.encode(),
        msg=f"{user_id}:{random_key}".encode(),
        digestmod=hashlib.sha256
    ).hexdigest()
    
    # Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙØªØ§Ø­ ÙˆØ§Ù„ØªÙˆÙ‚ÙŠØ¹
    api_key = f"wolf_{random_key}{signature[:10]}"
    
    # ØªØ®Ø²ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­
    expiry_date = datetime.now() + timedelta(days=expiry_days)
    API_KEYS[api_key] = {
        "user_id": user_id,
        "created_at": datetime.now().isoformat(),
        "expires_at": expiry_date.isoformat()
    }
    
    # Ø­ÙØ¸ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙÙŠ Ù…Ù„Ù
    api_keys_file = os.path.join(os.environ.get("API_KEYS_DIR"), "api_keys.json")
    with open(api_keys_file, "w") as f:
        json.dump(API_KEYS, f, indent=2)
    
    return api_key

# Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ù…ÙØªØ§Ø­ API
async def verify_api_key(api_key: str = Header(...)):
    if api_key not in API_KEYS:
        raise HTTPException(status_code=401, detail="Ù…ÙØªØ§Ø­ API ØºÙŠØ± ØµØ§Ù„Ø­")
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ§Ø±ÙŠØ® Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
    expiry_date = datetime.fromisoformat(API_KEYS[api_key]["expires_at"])
    if datetime.now() > expiry_date:
        raise HTTPException(status_code=401, detail="Ù…ÙØªØ§Ø­ API Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©")
    
    return api_key

# Ø¯Ø§Ù„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ©
def generate_mindmap(text: str) -> Dict[str, Any]:
    # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ
    # ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§
    
    lines = text.split("\n")
    nodes = []
    edges = []
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    title = "Wolf AI Mindmap"
    for line in lines:
        if line.startswith("# "):
            title = line[2:].strip()
            break
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¹Ù‚Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    nodes.append({
        "id": 0,
        "label": title,
        "depth": 0
    })
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§Ù„Ø­ÙˆØ§Ù
    current_id = 1
    for line in lines:
        if line.startswith("- "):
            content = line[2:].strip()
            nodes.append({
                "id": current_id,
                "label": content,
                "depth": 1
            })
            edges.append({
                "source": 0,
                "target": current_id
            })
            current_id += 1
        elif line.startswith("  - "):
            content = line[4:].strip()
            parent_id = max(1, current_id - 1)
            nodes.append({
                "id": current_id,
                "label": content,
                "depth": 2
            })
            edges.append({
                "source": parent_id,
                "target": current_id
            })
            current_id += 1
    
    return {
        "title": title,
        "nodes": nodes,
        "edges": edges
    }

# Ù…Ø³Ø§Ø± Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ API
@app.post("/api/keys/generate")
async def create_api_key(user_id: str, days: int = 365):
    api_key = generate_api_key(user_id, days)
    return {"api_key": api_key}

# Ù…Ø³Ø§Ø± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Øµ
@app.post("/api/completions", response_model=CompletionResponse)
async def create_completion(request: CompletionRequest, api_key: str = Depends(verify_api_key)):
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True
            )
        
        # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_text = generated_text[len(request.prompt):].strip()
        

# Ø¯Ø§Ù„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ©
def generate_mindmap(text: str) -> Dict[str, Any]:
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    concepts = []
    for sentence in text.split(". "):
        if len(sentence) > 20:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø©
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒÙ…Ø±Ø´Ø­ÙŠÙ† Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ…
            words = [word for word in sentence.split() if len(word) > 3]
            if words:
                concepts.append(words[0])
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ©
    nodes = [{"id": 0, "label": "Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "depth": 0}]
    edges = []
    
    for i, concept in enumerate(concepts[:5]):  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 5 Ù…ÙØ§Ù‡ÙŠÙ…
        node_id = i + 1
        nodes.append({
            "id": node_id,
            "label": concept,
            "depth": 1
        })
        edges.append({
            "source": 0,
            "target": node_id
        })
    
    return {
        "title": "Ø§Ù„Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø°Ù‡Ù†ÙŠØ©",
        "nodes": nodes,
        "edges": edges,
        "summary": "ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ"
    }

# Ù…Ø³Ø§Ø± Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ API
@app.post("/api/keys/generate")
async def create_api_key(user_id: str, days: int = 365):
    api_key = generate_api_key(user_id, days)
    return {"api_key": api_key}

# Ù…Ø³Ø§Ø± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ù†Øµ
@app.post("/api/completions", response_model=CompletionResponse)
async def create_completion(request: CompletionRequest, api_key: str = Depends(verify_api_key)):
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True
            )
        
        # ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response_text = generated_text[len(request.prompt):].strip()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø°Ù‡Ù†ÙŠØ© Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø°Ù„Ùƒ
        mindmap = None
        if request.include_mindmap:
            mindmap = generate_mindmap(response_text)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ù…ÙˆØ²
        input_tokens = len(inputs.input_ids[0])
        output_tokens = len(outputs[0]) - input_tokens
        total_tokens = input_tokens + output_tokens
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response = CompletionResponse(
            id=f"cmpl-{uuid.uuid4().hex}",
            object="text_completion",
            created=int(time.time()),
            model=config["model_name"],
            choices=[{
                "text": response_text,
                "index": 0,
                "logprobs": None,
                "finish_reason": "length"
            }],
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": total_tokens
            },
            mindmap=mindmap
        )
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Ù…Ø³Ø§Ø± Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ØªØ¯ÙÙ‚
@app.post("/api/completions/stream")
async def stream_completion(request: CompletionRequest, api_key: str = Depends(verify_api_key)):
    try:
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªØ¯ÙÙ‚
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
        
        # Ø¨Ø¯Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ ÙÙŠ Ø®Ù„ÙÙŠØ©
        generation_kwargs = dict(
            **inputs,
            streamer=streamer,
            max_new_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            do_sample=True
        )
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªØ¯ÙÙ‚Ø©
        async def response_generator():
            start_time = time.time()
            buffer = []
            
            for new_text in streamer:
                buffer.append(new_text)
                if len(buffer) > 3:  # Ø¥Ø±Ø³Ø§Ù„ ÙƒÙ„ 3 Ø±Ù…ÙˆØ²
                    chunk = ''.join(buffer)
                    yield f"data: {json.dumps({'text': chunk})}\n\n"
                    buffer = []
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø£ÙŠ Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ¨Ù‚ÙŠ
            if buffer:
                chunk = ''.join(buffer)
                yield f"data: {json.dumps({'text': chunk})}\n\n"
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø­Ø¯Ø« Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
            elapsed = time.time() - start_time
            yield f"data: [DONE]\n\n"
        
        return StreamingResponse(response_generator(), media_type="text/event-stream")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Ù…Ø³Ø§Ø± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
@app.get("/api/model/info")
async def get_model_info(api_key: str = Depends(verify_api_key)):
    return {
        "model_name": config["model_name"],
        "base_model": config["base_model"],
        "version": config.get("version", "1.0.0"),
        "parameters": f"{model.config.vocab_size}M",
        "description": "Ù†Ù…ÙˆØ°Ø¬ Wolf AI Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù„ØºÙˆÙŠ"
    }

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… Ø¹Ù†Ø¯ ØªÙ†ÙÙŠØ° Ø§Ù„Ø³ÙƒØ±Ø¨Øª Ù…Ø¨Ø§Ø´Ø±Ø©
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOL
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±Ø¨Øª ØªØ´ØºÙŠÙ„ API
    cat > $TRAINING_DIR/run_wolf_api.sh << 'EOL'
#!/bin/bash
export TRAINING_DIR="$HOME/wolf_ai_training"
export CONFIG_FILE="$TRAINING_DIR/wolf_config.json"
export API_KEYS_DIR="$TRAINING_DIR/api_keys"
export WOLF_API_SECRET="your_secure_secret_here"  # ÙŠØ¬Ø¨ ØªØºÙŠÙŠØ±Ù‡ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬

cd $TRAINING_DIR
python3 wolf_api.py
EOL
    
    # Ø¬Ø¹Ù„ Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ù‚Ø§Ø¨Ù„Ø§Ù‹ Ù„Ù„ØªÙ†ÙÙŠØ°
    chmod +x $TRAINING_DIR/run_wolf_api.sh
    
    echo -e "${GREEN}[+] ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© API Ø¨Ù†Ø¬Ø§Ø­${NC}"
    echo -e "${YELLOW}[!] Ù„ØªÙ†Ø´ÙŠØ· Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©: ${TRAINING_DIR}/run_wolf_api.sh${NC}"
    echo -e "${YELLOW}[!] Ø³ØªØ¹Ù…Ù„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø¹Ù„Ù‰: http://localhost:8000${NC}"
}

# ======================================================
# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®Ø·ÙˆØ§Øª
# ======================================================
main() {
    echo -e "${GOLD}"
    echo "======================================================"
    echo "           ğŸº Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Ø¨Øª Wolf AI"
    echo "======================================================"
    echo -e "${NC}"
    
    setup_directories
    install_dependencies
    download_datasets
    prepare_data
    create_mindmap_templates
    download_base_model
    finetune_model
    enhance_mindmap_capabilities
    evaluate_model
    merge_and_optimize
    create_api
    
    echo -e "${GOLD}"
    echo "======================================================"
    echo "       âœ… Ø§ÙƒØªÙ…Ù„ ØªÙ†ÙÙŠØ° Ø³ÙƒØ±Ø¨Øª Wolf AI Ø¨Ù†Ø¬Ø§Ø­!"
    echo "======================================================"
    echo -e "${NC}"
}

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
main
